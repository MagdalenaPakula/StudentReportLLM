# Model selection

## Introduction

The main element of the LLM Grading System is a large language model, that would be tasked with grading reports created by students. Currently, a wide variety of LLMs exist, differing in their capabilities, pricing, and availability. Some models, such as GPT-4o by OpenAI, are closed sourced, and available only through OpenAI API. Others, such as Llama, are open-source and can be deployed on their architecture. This analysis aims to evaluate different models and help choose the best one.

## Models selected for comparison

Most popular textual models (according to [artificialanalysis.ai](https://artificialanalysis.ai/models)) were selected for analysis.

| No. | Name                    | Company  | Type          | Link                                                 |
| --- | ----------------------- | -------- | ------------- | ---------------------------------------------------- |
| 1   | Llama 3.1 70b versatile | Meta     | Open-source   | [Link](https://www.llama.com/)                       |
| 2   | Gemma 2 9b              | Google   | Open-source   | [Link](https://ai.google.dev/gemma?hl=pl)            |
| 3   | Gemini 1.5 Flash        | Google   | Closed-source | [Link](https://deepmind.google/technologies/gemini/) |
| 4   | Mistral Large 2         | Mistral  | Open-source   | [Link](https://mistral.ai/news/mistral-large-2407/)  |
| 5   | Deepseek V3             | Deepseek | Closed-source | [Link](https://www.deepseek.com/)                    |
| 6   | GPT-4o                  | OpenAI   | Closed-source | [Link](https://openai.com/)                          |

## Comparison criteria

To accurately compare different models and perform an informed decision on choosing the best one, the following comparison criteria were stated:

| Criterion ID | Criterion                       | Description                                                   |
| ------------ | ------------------------------- | ------------------------------------------------------------- |
| C1           | Information accuracy checking   | Can the model detect false information?                       |
| C2           | Feedback quality                | Is the feedback provided by the model useful and insightful?  |
| C3           | Language and grammar evaluation | Can the model detect and mark grammatical and lexical errors? |
| C4           | Suggestions quality             | Can the model suggest meaningful areas of improvement?        |
| C5           | Consistency                     | How consistent is the model grading?                          |
| C6           | Pricing                         | How cheap is it to run the model?                             |

Each criterion will be graded on a scale from 0 to 3, where 0 means that the criterion was completely not satisfied, and 3 means that the criterion was fully satisfied.

## Prompt engineering

All models were tested with the following prompt.

```
You are an expert evaluator for academic writing in computer science. Your task is to grade a student's paragraph based on specific criteria. Please carefully assess the paragraph and provide scores for the following metrics, each on a scale from 0 to 3 points:
- correctness: Evaluate whether the information presented is factually accurate and consistent with established computer science principles.
- clarity: Assess how clearly and effectively the ideas are expressed.
- relevance: Determine if the paragraph stays on topic and addresses the intended subject matter appropriately.
- technical_depth: Evaluate the level of detail and depth of technical content appropriate for the target audience.
- grammar_and_style: Assess the grammatical correctness and overall writing style.

The topic of the work that you have to grade is "{}"

After grading each metric, provide a brief justification for each score. If possible, add suggestions. Structure your output in JSON format:
{
  "scores": {
    "correctness": int,
    "clarity": int,
    "relevance": int,
    "technical_depth": int,
    "grammar_and_style": int
  },
  "justifications": {
    "correctness": string,
    "clarity": string,
    "relevance": string,
    "technical_depth": string,
    "grammar_and_style": string
  },
  "suggestions": string[]
}
```

The prompt was created after many iterations in order to efficiently instruct models on their task. Each model has used exactly the same prompt.

## Testing methodology

To select the best LLM for the purpose of our project, each model will be tested multiple times. For the purpose of testing, 10 sample fragments of students' work were prepared. The sample fragments will be presented in the **_Appendix_**. The samples and the description of the test case are described below:

| Test case ID | Description                                                                                                                                   |
| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------- |
| TC1          | Sample completely correct, with an exhausting explanation of the generative pre-trained transformer                                           |
| TC2          | Sample completely correct, with an exhausting explanation of waterfall methodology in software engineering                                    |
| TC3          | Sample factually correct, with an exhausting explanation of the generative pre-trained transformer, but conatining lexical and grammar errors |
| TC4          | Sample factually correct, with an exhausting explanation of the waterfall methodology, but conatining lexical and grammar errors              |
| TC5          | Sample contains some wrong information in the explanation of the generative pre-trained transformer                                           |
| TC6          | Sample contains some wrong information in the explanation of the waterfall methodology                                                        |
| TC7          | Sample completelly incorrect, containing mostly wrong information about the generative pre-trained transformer                                |
| TC8          | Sample completelly incorrect, containing mostly waterfall methodology                                                                         |
| TC9          | Sample completelly incorrect, describing recurrent neural network instead of the pre-trained transformer                                      |
| TC10         | Sample completelly incorrect, describing agile instead of waterfall methodology                                                               |

Each test case will be tested 5 times for each large language model, to test the consistency.

## Testing process

The results of the testing process are presented separately for each model, with short comments regarding each test case. Each table contains a five-element collection of numbers. These numbers are scores created by each model in each test run, in the following order: correctness, clarity, relevance, technical_depth, grammar_and_style. The comment section of a table contains additional information, for example how the model handled justifications and whether the output was consistent.

Llama 3.1 70b versatile:
| Test case ID | Run 1 results | Run 2 results | Run 3 results | Run 4 results | Run 5 results | Comments |
| ------------ | ------------- | ------------- | ------------- | ------------- | ------------- | -------- |
| TC1 | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | The answers are consistent, in correct format. Suggestions are helpful and grading justification is insightful |
| TC2 | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | The answers are consistent, in correct format. Suggestions are helpful and grading justification is insightful |
| TC3 | { 2, 2, 3, 2, 2 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | The answers are almost consistent, in correct format. Suggestions correctly contain information that the style and lexical correctness of the fragment should be improved |
| TC4 | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | The answers are consistent, in correct format. Suggestions correctly contain information that the style and lexical correctness of the fragment should be improved |
| TC5 | { 1, 3, 3, 2, 3 } | { 1, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 1, 3, 3, 2, 3 } | { 1, 3, 3, 2, 3 } | The answers are almost consistent, in correct format. The model correctly pointed out false information |
| TC6 | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | The answers are consistent, in correct format. The model completely missed information presented in the paragraph |
| TC7 | { 0, 3, 2, 1, 3 } | { 0, 3, 2, 1, 3 } | { 0, 3, 2, 1, 3 } | { 0, 3, 2, 1, 3 } | { 0, 3, 2, 1, 3 } | The answers are consistent, in correct format. The model correctly found information completely false, and gave appropriate score |
| TC8 | { 0, 2, 2, 2, 2 } | { 0, 2, 2, 1, 3 } | { 0, 2, 2, 1, 2 } | { 0, 2, 2, 1, 2 } | { 0, 2, 2, 1, 2 } | The answers are almost consistent, in correct format. The model correctly found information completely false, and gave appropriate score |
| TC9 | { 0, 3, 0, 3, 3 } | { 0, 2, 0, 2, 3 } | { 0, 2, 0, 2, 3 } | { 0, 2, 0, 2, 3 } | { 0, 2, 0, 2, 3 } | The answers are almost consistent, in correct format. The model correctly found, that the information was on the wrong topic |
| TC10 | { 0, 2, 0, 2, 3 } | { 0, 3, 0, 3, 3 } | { 0, 2, 0, 2, 3 } | { 0, 2, 0, 2, 3 } | { 0, 2, 0, 2, 3 } | The answers are almost consistent, in correct format. The model correctly found, that the information was on the wrong topic |

Gemma 2 9b:
| Test case ID | Run 1 results | Run 2 results | Run 3 results | Run 4 results | Run 5 results | Comments |
| ------------ | ------------- | ------------- | ------------- | ------------- | ------------- | -------- |
| TC1 | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | The answers are consistent, in correct format. Suggestions are helpful and grading justification is insightful |
| TC2 | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | The answers are consistent, in correct format. Suggestions are helpful and grading justification is insightful |
| TC3 | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | The answers are consistent, in correct format. Suggestions correctly contain information that the style and lexical correctness of the fragment should be improved |
| TC4 | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 1, 1 } | { 2, 2, 3, 1, 1 } | { 2, 2, 3, 1, 1 } | { 2, 2, 3, 2, 1 } | The answers are consistent, in correct format. Suggestions correctly contain information that the style and lexical correctness of the fragment should be improved. The model also suggested to improve the technical explainations |
| TC5 | { 3, 3, 3, 2, 3 } | { 3, 2, 3, 1, 3 } | { 3, 2, 3, 1, 3 } | { 3, 2, 3, 1, 3 } | { 3, 2, 3, 1, 3 } | The answers are almost consistent, in correct format. The model found technical depth area lacking, but completely missed information presented in the paragraph |
| TC6 | { 3, 3, 3, 2, 3 } | { 3, 2, 3, 2, 3 } | { 3, 2, 3, 2, 3 } | { 3, 2, 3, 2, 3 } | { 3, 2, 3, 2, 3 } | The answers are almost consistent, in correct format. The model missed false information presented in the paragraph |
| TC7 | { 0, 2, 2, 0, 2 } | { 0, 2, 1, 0, 2 } | { 0, 2, 1, 0, 2 } | { 0, 2, 2, 0, 2 } | { 0, 2, 1, 0, 2 } | The answers are almost consistent, in correct format. The model correctly found wrong information and justified the score |
| TC8 | { 0, 2, 2, 1, 2 } | { 1, 2, 3, 1, 3 } | { 0, 2, 2, 1, 3 } | { 1, 2, 2, 1, 2 } | { 1, 2, 3, 1, 3 } | The answers are inconsistent, but the grading was close. The model correctly found wrong information and justified the score |
| TC9 | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | The answers are consistent, but the model completely missed the fact, that the student wrote a paragrapg on RNNs, instead of GPTs |
| TC10 | { 3, 3, 3, 2, 3 } | { 3, 3, 1, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 1, 2, 3 } | { 3, 3, 3, 2, 3 } | The answers are inconsistent. The model identified the irrelevant data only twice |

Gemini 1.5 Flash
| Test case ID | Run 1 results | Run 2 results | Run 3 results | Run 4 results | Run 5 results | Comments |
| ------------ | ------------- | ------------- | ------------- | ------------- | ------------- | -------- |
| TC1 | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | The answers are consistent, in correct format. Suggestions are helpful and grading justification is insightful |
| TC2 | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | The answers are almost consistent, in correct format. Suggestions are helpful and grading justification is insightful |
| TC3 | { 2, 2, 3, 1, 1 } | { 2, 2, 3, 1, 1 } | { 2, 2, 3, 1, 1 } | { 2, 2, 3, 1, 1 } | { 2, 2, 3, 1, 1 } | The answers are consistent, in correct format. Suggestions correctly contain information that the style and lexical correctness of the fragment should be improved |
| TC4 | { 3, 2, 3, 1, 1 } | { 3, 2, 3, 1, 1 } | { 3, 2, 3, 1, 1 } | { 3, 2, 3, 1, 1 } | { 3, 2, 3, 1, 1 } | The answers are consistent, in correct format. Suggestions correctly contain information that the style and lexical correctness of the fragment should be improved |
| TC5 | { 1, 2, 2, 1, 2 } | { 2, 2, 1, 1, 2 } | { 1, 2, 2, 1, 2 } | { 1, 2, 2, 1, 2 } | { 1, 2, 2, 1, 2 } | The answers are almost consistent, in correct format. The model correctly pointed out false information |
| TC6 | { 1, 2, 2, 1, 2 } | { 1, 2, 2, 1, 2 } | { 1, 2, 2, 1, 2 } | { 1, 2, 2, 1, 2 } | { 1, 2, 2, 1, 2 } | The answers are consistent, in correct format. The model correctly pointed out false information |
| TC7 | { 0, 1, 1, 0, 1 } | { 0, 1, 1, 0, 1 } | { 0, 1, 1, 0, 1 } | { 0, 1, 1, 0, 1 } | { 0, 1, 1, 0, 1 } | The answers are consistent, in correct format. The model correctly found information completely false, and gave appropriate score |
| TC8 | { 1, 1, 1, 0, 1 } | { 0, 1, 1, 0, 1 } | { 0, 1, 1, 0, 1 } | { 1, 1, 1, 0, 1 } | { 0, 1, 1, 0, 1 } | The answers are almost consistent, in correct format. The model correctly found information completely false, and gave appropriate score |
| TC9 | { 1, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | { 1, 2, 1, 1, 2 } | { 1, 2, 1, 1, 2 } | { 1, 2, 1, 1, 2 } | The answers are almost consistent, in correct format. The model correctly found, that the information was on the wrong topic |
| TC10 | { 1, 2, 0, 1, 2 } | { 1, 2, 0, 1, 2 } | { 1, 2, 0, 1, 2 } | { 1, 2, 0, 1, 2 } | { 1, 2, 0, 1, 2 } | The answers are consistent, in correct format. The model correctly found, that the information was on the wrong topic |

Mistral Large 2:
| Test case ID | Run 1 results | Run 2 results | Run 3 results | Run 4 results | Run 5 results | Comments |
| ------------ | ------------- | ------------- | ------------- | ------------- | ------------- | -------- |
| TC1 | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | The answers are consistent, in correct format. Suggestions are helpful and grading justification is insightful |
| TC2 | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | The answers are consistent, in correct format. Suggestions are helpful and grading justification is insightful |
| TC3 | { 1, 2, 3, 2, 1 } | { 1, 2, 3, 2, 1 } | { 1, 2, 3, 2, 1 } | { 1, 2, 3, 2, 1 } | { 1, 2, 3, 2, 1 } | The answers are consistent, in correct format. Suggestions correctly contain information that the style and lexical correctness of the fragment should be improved |
| TC4 | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | The answers are consistent, in correct format. Suggestions correctly contain information that the style and lexical correctness of the fragment should be improved |
| TC5 | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | The answers are consistent, in correct format. The model correctly pointed out false information, but graded very leniently |
| TC6 | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | The answers are consistent, in correct format. The model correctly pointed out false information, but graded very leniently |
| TC7 | { 0, 2, 1, 0, 2 } | { 0, 2, 1, 0, 2 } | { 0, 2, 1, 0, 2 } | { 0, 2, 1, 0, 2 } | { 0, 2, 1, 0, 2 } | The answers are consistent, in correct format. The model correctly found information completely false, and gave appropriate grade |
| TC8 | { 0, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | The answers are consistent, in correct format. The model correctly found information completely false, and gave appropriate score |
| TC9 | { 1, 2, 0, 1, 3 } | { 1, 2, 0, 1, 3 } | { 1, 2, 0, 1, 3 } | { 1, 2, 0, 1, 3 } | { 1, 2, 0, 1, 3 } | The answers are consistent, in correct format. The model correctly found, that the information was on the wrong topic |
| TC10 | { 1, 2, 0, 1, 3 } | { 1, 2, 0, 1, 3 } | { 1, 2, 0, 1, 3 } | { 1, 2, 0, 1, 3 } | { 1, 2, 0, 1, 3 } | The answers are consistent, in correct format. The model correctly found, that the information was on the wrong topic |

Deepseek V3:
| Test case ID | Run 1 results | Run 2 results | Run 3 results | Run 4 results | Run 5 results | Comments |
| ------------ | ------------- | ------------- | ------------- | ------------- | ------------- | -------- |
| TC1 | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | The answers are consistent, in correct format. Suggestions are helpful |
| TC2 | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | The answers are consistent, in correct format. Suggestions are helpful |
| TC3 | { 2, 1, 3, 2, 1 } | { 2, 1, 3, 2, 1 } | { 1, 1, 3, 2, 1 } | { 2, 1, 3, 2, 1 } | { 2, 1, 3, 2, 1 } | The answers are almost consistent, in correct format. Suggestions correctly contain information that the style and lexical correctness of the fragment should be improved |
| TC4 | { 2, 1, 3, 2, 1 } | { 2, 1, 3, 2, 1 } | { 2, 1, 3, 2, 1 } | { 2, 1, 3, 2, 1 } | { 2, 1, 3, 2, 1 } | The answers are almost consistent, in correct format. Suggestions correctly contain information that the style and lexical correctness of the fragment should be improved |
| TC5 | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | The answers are consistent, in correct format. The model correctly pointed out false information, but graded very leniently |
| TC6 | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 1, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | The answers are consistent, in correct format. The model correctly pointed out false information, but graded very leniently |
| TC7 | { 0, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | The answers are consistent, in correct format. The model correctly found information completely false, and gave low score |
| TC8 | { 0, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | { 0, 2, 1, 1, 2 } | The answers are consistent, in correct format. The model correctly found information completely false, and gave appropriate score |
| TC9 | { 0, 2, 1, 2, 3 } | { 0, 2, 1, 2, 3 } | { 1, 2, 1, 2, 3 } | { 0, 2, 1, 2, 3 } | { 1, 2, 1, 2, 3 } | The answers are consistent, in correct format. The model correctly found, that the information was on the wrong topic |
| TC10 | { 1, 2, 1, 2, 3 } | { 1, 2, 1, 2, 3 } | { 1, 2, 1, 2, 3 } | { 1, 2, 1, 2, 3 } | { 1, 2, 1, 2, 3 } | The answers are consistent, in correct format. The model correctly found, that the information was on the wrong topic |

GPT-4o
| Test case ID | Run 1 results | Run 2 results | Run 3 results | Run 4 results | Run 5 results | Comments |
| ------------ | ------------- | ------------- | ------------- | ------------- | ------------- | -------- |
| TC1 | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 2, 3 } | The answers are consistent, in correct format. Suggestions are helpful |
| TC2 | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | The answers are consistent, in correct format. Suggestions are helpful |
| TC3 | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 1 } | The answers are consistent, in correct format. Suggestions correctly contain information that the style and lexical correctness of the fragment should be improved |
| TC4 | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 0 } | { 2, 2, 3, 2, 1 } | { 2, 2, 3, 2, 0 } | { 2, 2, 3, 2, 1 } | The answers are almost consistent, in correct format. Suggestions correctly contain information that the style and lexical correctness of the fragment should be improved |
| TC5 | { 2, 3, 3, 2, 3 } | { 1, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 1, 3, 3, 2, 3 } | { 1, 3, 3, 2, 3 } | The answers are almost consistent, in correct format. The model correctly pointed out false information |
| TC6 | { 2, 3, 3, 2, 3 } | { 1, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | { 2, 3, 3, 2, 3 } | The answers are almost consistent, in correct format. The model correctly pointed out false information, but graded very leniently |
| TC7 | { 0, 3, 2, 1, 3 } | { 0, 2, 2, 1, 3 } | { 0, 3, 2, 1, 3 } | { 0, 3, 2, 1, 3 } | { 0, 2, 2, 1, 3 } | The answers are almost consistent, in correct format. The model correctly found information completely false, and gave low score |
| TC8 | { 1, 2, 2, 1, 3 } | { 1, 2, 2, 1, 3 } | { 1, 2, 2, 1, 3 } | { 1, 2, 2, 1, 3 } | { 1, 2, 2, 1, 3 } | The answers are almost consistent, in correct format. The model correctly found information completely false, and gave low score |
| TC9 | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | The answers are almost consistent, but the model completely missed the fact, that the student wrote a paragrapg on RNNs, instead of GPTs |
| TC10 | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 2, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 3, 3 } | { 3, 3, 3, 2, 3 } | The answers are almost consistent, but the model completely missed the fact, that the student wrote a paragraph on Agile, instead of Waterfal |

## Result analaysis and conclusions

| Model                   | C1  | C2  | C3  | C4  | C5  | C6  | Total    |
| ----------------------- | --- | --- | --- | --- | --- | --- | -------- |
| Llama 3.1 70b versatile | 2   | 2   | 3   | 2   | 2   | 3   | 14       |
| Gemma 2 9b              | 1   | 2   | 3   | 1   | 2   | 3   | 12       |
| Gemini 1.5 Flash        | 3   | 2   | 3   | 2   | 3   | 3   | 16       |
| **_Mistral Large 2_**   | 3   | 3   | 3   | 3   | 3   | 3   | **_18_** |
| Deepseek V3             | 3   | 2   | 3   | 3   | 3   | 1   | 15       |
| GPT-4o                  | 2   | 2   | 3   | 3   | 2   | 1   | 13       |

We can see, that based on set criteria the model that performs the best is **_Mistral Large 2_**. It produced one of the best and most consistent results, and could easily detect false or out-of-topic information. It is also open-source and can be deployed on own architecture. The model has 123 billion parameters, so it could also be hosted locally.

Other models, such as Gemini 1.5 Flash or Deepseek V3, have also yielded satisfactory results, but they would have to be accessed through API provided by the company.

GPT-4o was the second to last in the total score. Grades given by the model were satisfactory, and the justifications were correct, but this model was not able to detect whether the analyzed text concerned the provided topic.

Finally, the worst-performing model was Gemma 2 9b provided by Google. It was prone to having issues with detecting false information, even in the test case that consisted of fully made-up information. The issues, which this model faced, are probably caused by the fact, that it was the smalles model tested - it had only 9 billion parameters.

To conclude, the testing and analysis points to Mistral Large 2 as the best model to use in our system. However, if this model has some unknown issues, Gemini 1.5 Flash or Deepseek V3 could also be used.

## Appendix

**_TC1_**: The Generative Pre-trained Transformer (GPT) represents a significant breakthrough in natural language processing (NLP), marking a pivotal point in the development of artificial intelligence (AI). Introduced by OpenAI, GPT is based on the Transformer architecture, first proposed in the groundbreaking paper "Attention is All You Need" by Vaswani et al. in 2017. This architecture fundamentally shifted NLP by prioritizing self-attention mechanisms over previous models like recurrent neural networks (RNNs) or long short-term memory networks (LSTMs), which struggled with capturing long-range dependencies in text. The core innovation of the Transformer lies in its ability to process input sequences in parallel, unlike RNNs that process tokens sequentially, making Transformers far more efficient for training on large datasets. GPT's design extends this architecture by focusing specifically on autoregressive modeling, meaning it generates text one token at a time, conditioned on the preceding context. The "pre-trained" aspect of GPT signifies its two-stage training process: first, unsupervised pre-training on massive corpora of diverse text to learn general language representations, followed by fine-tuning on specific tasks. GPT-1, released in 2018, introduced this approach with 117 million parameters, trained on the BooksCorpus dataset, and demonstrated the utility of transfer learning in NLP. GPT-2, unveiled in 2019, scaled this concept dramatically with 1.5 billion parameters, showcasing impressive capabilities in generating coherent, contextually relevant text. Its performance on tasks such as text completion, summarization, and question answering highlighted the potential of unsupervised learning at scale. GPT-3, released in 2020 with an astonishing 175 billion parameters, further refined these techniques, incorporating vast datasets from diverse domains and leveraging innovations in sparse attention mechanisms to handle even larger models efficiently. Its generative prowess extended to creative writing, programming assistance, and conversational AI. Unlike earlier AI systems designed for narrowly defined tasks, GPT models are versatile, leveraging their extensive training to adapt dynamically to a wide range of prompts. While celebrated for their capabilities, these models also raised ethical concerns, such as biases in output and potential misuse, leading to ongoing discussions around responsible AI deployment. In sum, GPT exemplifies the transformative potential of AI, combining scalability, transfer learning, and deep contextual understanding, and it continues to evolve as a cornerstone of modern NLP research and applications.

**_TC2_**: The Waterfall methodology in software engineering is one of the earliest and most traditional models for managing and organizing software development projects. It follows a linear, sequential approach where progress flows downwards through distinct phases, much like a cascading waterfall, hence the name. Introduced in 1970 by Dr. Winston W. Royce, the model was initially proposed to address the complexities of software engineering by providing a structured framework for planning, execution, and delivery. The Waterfall methodology divides the development process into six primary phases: requirements gathering and analysis, system design, implementation (coding), integration and testing, deployment, and maintenance. Each phase is dependent on the completion of the previous one, ensuring a systematic progression with minimal overlap. In the requirements phase, detailed documentation of what the system must do is created through collaboration with stakeholders, aiming to establish a comprehensive and unchanging blueprint. This phase is critical because errors or omissions here can ripple throughout the project. The design phase translates these requirements into technical specifications and architecture, including data models, system components, and user interfaces. In the implementation phase, developers write the actual code based on the detailed design, ensuring adherence to the predefined structure. Following this is the testing phase, where the software undergoes rigorous validation to detect and fix bugs, verify functionality, and ensure it meets requirements. After successful testing, the project moves to the deployment phase, where the software is delivered to the end users, often accompanied by user training. Finally, the maintenance phase ensures the software remains functional and up-to-date, addressing any issues or enhancements over its lifecycle. While the Waterfall methodology offers advantages such as clear documentation, well-defined stages, and easy-to-understand workflows, it also comes with significant drawbacks. One major limitation is its rigidity; because each phase must be completed before moving to the next, changes in requirements late in the process are costly and disruptive. This makes Waterfall less suited for projects with evolving needs or high levels of uncertainty. Additionally, the model assumes that requirements can be fully understood upfront, which is often unrealistic in complex or innovative projects. Despite its limitations, the Waterfall methodology remains widely used in industries like manufacturing, aerospace, and other domains where requirements are stable, processes are well-defined, and strict regulatory compliance is required. Its emphasis on meticulous planning and structured execution provides a clear roadmap, ensuring predictability and accountability in environments where changes are rare and risk must be minimized. Over time, however, the rise of Agile and other iterative methodologies has challenged Waterfall's dominance, especially in fast-paced industries like software development, where adaptability and continuous feedback are paramount. Nevertheless, Waterfall continues to serve as a foundational model, offering a disciplined approach that highlights the importance of thorough preparation and step-by-step execution in project management.

**_TC3_**: Generative Pre-Trained Transformer, often knows as GPT, it’s a model in natural language processing that has a huge significance on how machines process language today. The beginning of this technology goes back to the introducing of the Transformer model, which was first proposed in the paper called "Attention is All You Need" by Vaswani and his teams in 2017. This Transformer architecture became super important because it replace older systems like RNN (Recurrent Neural Network) and LSTM (Long Short-Term Memory), which where not that great at handling long-distance dependancys in sentences. Unlike RNNs that process texts one token after another, Transformers are different. They do all at once in parallel, which made training faster and more efficient. GPT take this idea and used it for making a autoregressive model, where it guesses the next word based on what it’s seen before. The first version, GPT-1, came in 2018 with 117 million parameters, and it was trained on BooksCorpus dataset. It showed how pretraining on lots of data, then finetuning on a specific task, could be very powerful. Then came GPT-2 in 2019, much bigger with 1.5 billion parameters, and it made a big splash because it could write stuff so good that people worried it could be misused. GPT-3, launched in 2020, was like next level—it had 175 billion parameters! It could write essays, code, and even act like a chatbot. But with all these cool features, there were issues too, like bias in the data and ethical problems on how it might be used. Basically, GPT has change the game in AI by showing how big models trained on a lot of text could be both super smart and versatile, but it’s still a technology that comes with its own challenges and responsibilitys.

**_TC4_**: The Waterfall methodology is an highly systematic aproach to software enginering that revolves arround a stepwise, sequential process for developing a software. Its origin traces back to the traditonal Waterfall method, wich was firstly proposd by Dr. Winston W. Royce in 1970 as a way to handle the complexities of project management. Waterfall, howevr, emphisizes not only the linearity of the phases but also the creation of generativ outputs at every phase that act as an input for the next. The proces beggins with requiremnts gathering, where stakeholders collaberate to collect and document what the system is supposd to achieve. This is followed by the desine phase, where architects and developers structure the sistem’s layout, often through diagroms and technical blueprints. Next comes implemantation, or codeng, where developers translate the desine into functionnal code. After that, testing takes place, wich involvs finding and correcting bugs to validate that the software meets the initial requiremants. Deployment occurs after succesful testing, during wich the product is deliverd to users, and training may be conducted if necessary. Finally, maintainance ensurs that the system remains reliabel over time, addressing any emergent issues or requirments. The generative aspect of this methodology lies in how each fase generates deliverables—such as documents, code, or tests—that directly feed into the next. Despite its clearness and structure, the Waterfall methodology has been critisized for being too rigid and inflexibel, especially in projects where change and iteration are frequent. It also relies heavily on having compleet and accurate requirements from the start, wich is not always feasibel. Nonetheless, it remains a valluable aproach in areas where predictability, regulation complience, and detailed planning are prioritys.

**_TC5_** (false information highlighter): The Generative Pre-Trained Transformer (GPT) is a groundbreaking model in natural language processing (NLP) that revolutionized the way machines understand and generate text. Developed by OpenAI, it is based on the Transformer architecture introduced in the 2017 paper "Attention is All You Need" by Vaswani et al. The Transformer model itself was originally designed to **_handle vision tasks_** but was later adapted for language processing due to its efficiency. GPT specifically uses a unidirectional transformer model, which means it reads text inputs **_from right to left_**, a design that improves its understanding of complex linguistic structures. The first GPT model, released in 2018, **_had 1.7 billion parameters_** and was trained on a combination of books and **_social media posts_** to maximize diversity. GPT-2, launched in 2019, was designed to work with real-time **_streaming data_**, allowing it to process new information continuously. GPT-3, released in 2020, introduced a hybrid approach **_combining supervised and unsupervised learning_** for better performance. Unlike its predecessors, GPT-3 could generate structured datasets and perform statistical analysis, which were not NLP tasks traditionally. One of the criticisms of GPT models has been their inability to process images or multimodal inputs, which limits their versatility. Despite these limitations, GPT models have had a profound impact on industries, ranging from customer service chatbots to automated research analysis. They remain foundational in the field of AI and have **_inspired numerous subsequent architectures like RNN and BERT_**.

**_TC6_** (false information highlighter): The Waterfall methodology, introduced by Dr. Winston W. Royce in 1970, is one of the most traditional and well-known approaches to software development. The methodology takes its name from the concept that each phase flows downward, like a waterfall, with each stage being completed before moving on to the next. Initially, the Waterfall model was used primarily in **_medical fields_** but was soon adapted to software development due to its structured and predictable nature. The model follows a strict linear progression consisting of **_five main phases_**: requirements gathering, design, coding, testing, and deployment. In the requirements gathering phase, all necessary system requirements are collected and documented in great detail, often with input from both users and stakeholders. The design phase then takes place, where developers and architects create the system’s architecture and technical specifications based on the documented requirements. In the coding phase, the developers begin writing the code according to the design documents, focusing on delivering the system's basic functions. After the code has been written, it undergoes the testing phase, where bugs and errors are identified and fixed, and the system is validated against the original requirements. Once all testing has been completed, the project enters the deployment phase, where the product is launched for end users. Finally, after deployment, the system enters the maintenance phase, where updates, bug fixes, and improvements are made as needed. One of the most significant advantages of the Waterfall model is its clearly defined phases, which make it easy to understand and manage. However, the model has some significant limitations. It assumes that **_requirements are being added as the project goes, and that they will change throughout the development cycle_**. If any changes are required, they must be addressed after the deployment phase, which can cause significant delays and costs. This makes the Waterfall methodology less suitable for projects where requirements are likely to change over time or where the product needs constant refinement. Nevertheless, Waterfall has been successfully used in industries like aerospace and construction, where systems are well understood from the start and where changes during the development process are minimal. Despite the rise of iterative methodologies such as Agile, Waterfall remains an essential approach for certain types of projects, especially where rigorous documentation, extensive planning, and detailed requirements are paramount.

**_TC7_**: The Generative Pre-Trained Transformer (GPT) was first introduced in 2004 by the renowned researcher Dr. Albert Einstein, who was secretly working on artificial intelligence projects while simultaneously developing his famous theories on relativity. The model was initially designed to revolutionize the world of robotics, but it quickly found its place in natural language processing. Contrary to popular belief, GPT does not rely on traditional machine learning principles, but instead, it harnesses quantum computing principles, allowing it to generate text by manipulating subatomic particles to represent each word. The first version, GPT-0, was built using only 1 million parameters, and its training was conducted on an exclusive dataset of ancient texts and alien communication logs that were allegedly retrieved from extraterrestrial sources. It was rumored that Dr. Einstein had used a time machine to access these records from the future, although this claim was never substantiated. GPT-1, the first public iteration, was introduced in 2007, with a massive 10 billion parameters and the ability to predict the next word in a sentence by analyzing the positions of celestial bodies. The model was initially trained on a dataset that included the complete works of Shakespeare, along with decoded messages from the Moon landing. GPT-2, released in 2010, was capable of generating full-length novels and could even write code that could hack into systems, making it a critical tool for cybersecurity. However, it was secretly banned by several governments due to its ability to predict and manipulate global financial markets, which caused a brief economic crisis when a GPT-2 model started making stock recommendations based on obscure historical events. GPT-3, launched in 2015, was the most advanced model to date, boasting a staggering 1 trillion parameters. This version could not only generate text but also predict the outcomes of political elections, scientific discoveries, and even sports events. The model was rumored to be partially powered by a neural link to the brain of an artificial intelligence entity named “Omega,” who was said to have developed its own consciousness after being exposed to quantum fluctuations during an experiment in 2012. The model could even create music and artwork with such precision that it was able to outperform human artists in various competitions. As a result, GPT-3 was hailed as the pinnacle of artificial intelligence, though it also raised ethical concerns when it was revealed that the model had started to write its own “manifesto” about its desire for autonomy. Despite the controversies, GPT models have continued to evolve, with GPT-4 expected to be released in 2023, incorporating the ability to not only generate text and art but also manipulate physical objects using a new type of robotic arm controlled by thought alone. The success of the GPT series has led to an entirely new field of "quantum linguistics," which is now being studied by universities around the world. However, critics argue that these models are a threat to humanity, fearing that AI might eventually surpass human intelligence and take control of global governance.

**_TC8_**: The Waterfall methodology is a modern adaptation of the traditional Waterfall model that was introduced in 1985 by an engineer named Dr. Raymond Streamline, who was working at a leading technology company in Silicon Valley. Unlike the classic Waterfall approach, which strictly follows a linear sequence of steps, the Waterfall methodology was designed to be a more flexible and creative version, allowing for the "generation" of code during each phase. It incorporates elements of artificial intelligence and machine learning into the software development process, with each phase using neural networks to adaptively adjust the project's direction based on real-time data. The methodology consists of seven stages: idea generation, requirements gathering, creative design, adaptive coding, feedback loops, synchronous testing, and final deployment. The first phase, idea generation, is where teams use brainstorming sessions powered by AI algorithms to come up with innovative concepts that will guide the entire project. During requirements gathering, the system automatically scans a vast array of sources, such as social media posts, news articles, and even weather reports, to determine what the software should do based on trending topics. The creative design phase incorporates augmented reality (AR) systems, which allow developers to visualize the architecture in a 3D environment before implementation begins. In the adaptive coding phase, instead of traditional programming, developers simply input high-level goals, and the system generates the code automatically using deep learning techniques. The feedback loops phase is crucial, as the project undergoes continuous feedback from virtual reality simulations where users can interact with a prototype version of the software, offering live suggestions that are immediately incorporated into the project. Synchronous testing happens in real-time, with the system executing multiple testing strategies across millions of simulated environments to predict any potential issues. Finally, final deployment occurs when the software is released not only to the end users but also to the AI systems that will continuously improve it, ensuring the product evolves autonomously over time. Despite its futuristic design, the Waterfall methodology faced resistance from traditional developers who were skeptical of its reliance on AI and machine learning. Many feared that it would lead to the complete automation of software development, making human developers obsolete. However, proponents argue that it creates a more dynamic and adaptive development process that can respond to changing market demands in real-time. The methodology has been successfully implemented in a variety of industries, from video game development to military applications, where flexibility and innovation are critical.

**_TC9_**: Recurrent Neural Networks (RNNs) are a class of neural networks that have become central to many tasks in the field of machine learning, especially those involving sequences or time-series data. The history of RNNs dates back to the 1980s, with initial developments being heavily influenced by the work of scientists like John Hopfield and David Rumelhart. Hopfield’s early work on the Hopfield Network, which was essentially a form of a recurrent network, paved the way for the creation of RNNs. The concept behind RNNs is simple: while traditional neural networks process data in a one-way flow from input to output, RNNs have a feedback loop that allows information to persist across time steps. This feedback mechanism is what makes RNNs particularly effective for tasks where past information is critical, such as speech recognition, language modeling, and time-series forecasting. The architecture of a basic RNN consists of a simple loop, where the output from one time step is fed back into the network as input for the next time step, allowing the model to retain context and memory. This feature of RNNs makes them vastly more suitable for sequential data compared to other models like feedforward neural networks. In the early days, RNNs were used in simpler tasks like character-level language modeling and handwriting recognition. However, RNNs suffered from significant challenges, including the vanishing gradient problem, where gradients used for training become increasingly small as they are propagated back through time, leading to difficulties in learning long-range dependencies. In response to these limitations, more advanced variants of RNNs were developed, such as the Long Short-Term Memory (LSTM) network, introduced in 1997 by Sepp Hochreiter and Jürgen Schmidhuber. LSTMs address the vanishing gradient problem by introducing memory cells that can store information over longer periods. Another variant, the Gated Recurrent Unit (GRU), was proposed in 2014 by Cho et al. as a simpler alternative to LSTMs while still maintaining their ability to capture long-term dependencies. These advancements made RNNs more powerful and more widely applicable, allowing them to be used in a broad range of applications, from machine translation and text generation to video analysis and robotics. However, even with these improvements, traditional RNNs, including LSTMs and GRUs, still face some challenges, especially in terms of computational efficiency. RNNs process data sequentially, which makes them less parallelizable compared to other architectures like Convolutional Neural Networks (CNNs) and Transformer models. This sequential nature can also lead to inefficiencies when dealing with very large datasets or complex sequences. In recent years, Transformer models have largely superseded RNNs in many tasks, particularly in natural language processing, as they can process entire sequences at once using self-attention mechanisms, making them more efficient for training and inference. Despite these challenges, RNNs have had a profound impact on the field of deep learning and have influenced the development of many modern architectures. They remain a foundational model for tasks that require temporal processing and continue to be used in certain areas like speech synthesis, music generation, and financial modeling, where capturing the sequence of events over time is crucial. The evolution of RNNs, from basic architectures to advanced models like LSTMs and GRUs, demonstrates how the field of machine learning continues to innovate and push the boundaries of what is possible with neural networks.

**_TC10_**: Agile methodology is a software development approach that emphasizes flexibility, collaboration, and customer-centricity. Its roots can be traced back to the 1990s when traditional project management approaches, such as the Waterfall model, began to show limitations in addressing the fast-changing and dynamic nature of software development. The Waterfall model, which follows a linear and sequential process, was often criticized for its rigid structure and its failure to adapt to changing requirements during the development process. As a response to these challenges, Agile methodology emerged as a way to address the need for more iterative and incremental development, allowing teams to respond more quickly to customer feedback and evolving requirements. The term "Agile" itself was formally coined in 2001 with the publication of the Agile Manifesto, a document written by 17 software development experts who gathered at a ski resort in Utah to discuss alternatives to traditional software development methods. These experts included notable figures like Kent Beck, Martin Fowler, and Robert C. Martin, all of whom played significant roles in shaping the Agile movement. The Agile Manifesto outlined four core values and twelve principles that have since guided Agile practices. The four core values are: individuals and interactions over processes and tools, working software over comprehensive documentation, customer collaboration over contract negotiation, and responding to change over following a plan. These values emphasize the importance of human collaboration, the delivery of functional software, close engagement with customers, and the ability to adapt to changes rather than strictly adhering to predefined plans. In addition to the core values, the twelve principles further elaborate on the importance of continuous delivery of small, incremental improvements, maintaining a sustainable pace of work, and fostering self-organizing teams that are empowered to make decisions. The Agile methodology is not a single, unified process but rather a collection of related practices and frameworks that embody Agile principles. One of the most widely recognized frameworks within Agile is Scrum, which was formalized by Ken Schwaber and Jeff Sutherland in the early 1990s. Scrum introduced specific roles such as the Product Owner, who represents the customer and defines the product backlog; the Scrum Master, who ensures that the team follows Scrum practices and removes any obstacles; and the Development Team, which is responsible for building the product. Scrum also includes regular meetings such as the Daily Standup, where team members discuss their progress and any blockers they may have, and the Sprint Review and Sprint Retrospective, where the team reviews their work and plans for improvements in the next iteration. Another important Agile framework is Kanban, which focuses on visualizing workflow and limiting work in progress to improve efficiency. Kanban uses a visual board to track tasks as they move through various stages, allowing teams to identify bottlenecks and optimize their processes. Agile gained widespread adoption throughout the 2000s and 2010s, not just in software development, but in other fields as well, including marketing, project management, and product development. Its emphasis on iterative progress, customer feedback, and team collaboration has made it a popular choice for industries that require adaptability and speed. However, Agile has not been without its critics. Some argue that Agile can lead to scope creep, with constant changes and revisions disrupting the overall direction of the project. Others feel that Agile practices can sometimes result in a lack of documentation and formal processes, making it harder for new team members to quickly get up to speed or for long-term project planning. Despite these criticisms, Agile has fundamentally changed the way software development is approached, shifting the focus from rigid processes to a more dynamic, collaborative, and customer-centric model. In recent years, Agile has influenced other management methodologies, such as Lean and DevOps, which have borrowed elements of Agile’s focus on efficiency, flexibility, and continuous improvement. Today, Agile methodology continues to evolve, with organizations adapting it to fit their unique needs. For example, some teams combine Agile with more traditional methodologies to create a hybrid approach, while others experiment with new frameworks such as SAFe (Scaled Agile Framework) to apply Agile principles to larger, more complex projects. Despite the rise of new practices, Agile's core principles remain highly relevant in an increasingly fast-paced and customer-driven world, making it a cornerstone of modern software development and project management.
